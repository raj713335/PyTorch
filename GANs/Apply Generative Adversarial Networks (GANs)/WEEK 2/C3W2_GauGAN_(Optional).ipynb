{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C3W2: GauGAN (Optional).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# GauGAN\n",
        "\n",
        "*Please note that this is an optional notebook, meant to introduce more advanced concepts if you're up for a challenge, so don't worry if you don't completely follow!*\n",
        "\n",
        "It is recommended that you should already be familiar with:\n",
        " - Pix2PixHD, from [High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/abs/1711.11585) (Wang et al. 2018)\n",
        " - Synchronized batch norm. See Pytorch's [SyncBatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html) documentation.\n",
        " - Kullbach-Leibler divergence\n",
        "\n",
        "### Goals\n",
        "\n",
        "In this notebook, you will learn about GauGAN, which synthesizes high-resolution images from semantic label maps, which you implement and train. GauGAN is based around a special denormalization technique proposed in [Semantic Image Synthesis with Spatially-Adaptive Normalization](https://arxiv.org/abs/1903.07291) (Park et al. 2019)\n",
        "\n",
        "### Background\n",
        "GauGAN builds on Pix2PixHD but simplifies the overall network by adding spatially adaptive denormalization layers. Because it learns its denormalization parameters via convolving the instance segmentation map, it actually is better for multi-modal synthesis, since all it needs as is a random noise vector. Later in the notebook, you will see how the authors further control diversity with the noise vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnUBfO3P4vFm"
      },
      "source": [
        "## GauGAN Submodules\n",
        "\n",
        "Let's first take a look at the building blocks behind GauGAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-V4Q3J3QBYB"
      },
      "source": [
        "### Synchronized BatchNorm\n",
        "\n",
        "So you've already heard of batch norm, which is a normalization technique that tries to normalize the statistics of activations a standard Gaussian distribution.\n",
        "\n",
        "Batch norm, however, performs poorly with small batch sizes. This becomes a problem when training large models that can only fit small batch sizes on GPUs. Training on multiple GPUs will increase the effective batch size, but vanilla batch norm will only update its statistics asynchronously on each GPU. Essentially, if you train on 2 gpus with `nn.BatchNorm2d`, the two batchnorm modules will have a different running averages of statistics and batch norm stability isn't better from larger effective batch size.\n",
        "\n",
        "Synchronized batch norm ([nn.SyncBatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html)) does exactly what its name suggests - it synchronizes batch norm running average updates across multiple processes so that each update will be with the statistics across all your minibatches.\n",
        "\n",
        "The authors report slightly better scores with synchronized batch norm as opposed to regular (asynchronous) batch norm. Since you will likely be running this on one machine, this notebook will stick to regular `nn.BatchNorm2d` modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciGiwTnpQNL-"
      },
      "source": [
        "### Spatially Adaptive Denormalization (SPADE)\n",
        "\n",
        "Recall that normalization layers are formulated as\n",
        "\n",
        "\\begin{align*}\n",
        "    y &= \\dfrac{x - \\hat{\\mu}}{\\hat{\\sigma}} * \\gamma + \\beta\n",
        "\\end{align*}\n",
        "\n",
        "where $\\hat{\\mu}$ and $\\hat{\\sigma}$ correspond to an exponential moving average of minibatch means and standard deviations and are used to normalize the input activation $x$. The parameters $\\gamma$ and $\\beta$ apply \"denormalization,\" essentially allowing the model to invert the normalization if necessary.\n",
        "\n",
        "In GauGAN, batch norm is the preferred normalization scheme. Recall that batch norm can be formulated for each input neuron as\n",
        "\n",
        "\\begin{align*}\n",
        "    y_{c,h,w} &= \\dfrac{x_{c,h,w} - \\hat{\\mu}_c}{\\hat{\\sigma}_c} * \\gamma_c + \\beta_c\n",
        "\\end{align*}\n",
        "\n",
        "where $\\hat{\\mu}_c$ and $\\hat{\\sigma}_c$ are per-channel statistics computed across the batch and spatial dimensions. Similarly, $\\gamma_c$ and $\\beta_c$ are per-channel denormalization parameters.\n",
        "\n",
        "With vanilla batch norm, these denormalization parameters are spatially invariant - that is, the same values are applied to every position in the input activation. As you may imagine, this could be limiting for the model. Oftentimes it's conducive for the model to learn denormalization parameters for each position.\n",
        "\n",
        "The authors address this with **SPatially Adaptive DEnormalization (SPADE)**. They compute denormalization parameters $\\gamma$ and $\\beta$ by convolving the input segmentation masks and apply these elementwise. SPADE can therefore be formulated as\n",
        "\n",
        "\\begin{align*}\n",
        "    y_{c,h,w} &= \\dfrac{x_{c,h,w} - \\hat{\\mu}_c}{\\hat{\\sigma}_c} * \\gamma_{c,h,w} + \\beta_{c,h,w}\n",
        "\\end{align*}\n",
        "\n",
        "Now let's implement SPADE!\n",
        "\n",
        "Note: the authors use spectral norm in all convolutional layers in the generator and discriminator, but the official code omits spectral norm for SPADE layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCxv5rhWNhr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SPADE(nn.Module):\n",
        "    '''\n",
        "    SPADE Class\n",
        "    Values:\n",
        "        channels: the number of channels in the input, a scalar\n",
        "        cond_channels: the number of channels in conditional input (one-hot semantic labels), a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, cond_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batchnorm = nn.BatchNorm2d(channels)\n",
        "        self.spade = nn.Sequential(\n",
        "            nn.Conv2d(cond_channels, channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels, 2 * channels, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        # Apply normalization\n",
        "        x = self.batchnorm(x)\n",
        "\n",
        "        # Compute denormalization\n",
        "        seg = F.interpolate(seg, size=x.shape[-2:], mode='nearest')\n",
        "        gamma, beta = torch.chunk(self.spade(seg), 2, dim=1)\n",
        "\n",
        "        # Apply denormalization\n",
        "        x = x * (1 + gamma) + beta\n",
        "        return x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB1Vq8ps7Bfd"
      },
      "source": [
        "### Residual Blocks\n",
        "\n",
        "Let's now implement residual blocks with SPADE normalization. You should be familiar with the residual block by now, but this implementation will be a bit different to accomodate for the extra semantic label map input. For a refresher on residual blocks, please take a look [here](https://paperswithcode.com/method/residual-block)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHD_wif07f4b"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    '''\n",
        "    ResidualBlock Class\n",
        "    Values:\n",
        "        in_channels: the number of input channels, a scalar\n",
        "        out_channels: the number of output channels, a scalar\n",
        "        cond_channels: the number of channels in conditional input in spade layer, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, cond_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        hid_channels = min(in_channels, out_channels)\n",
        "\n",
        "        self.proj = in_channels != out_channels\n",
        "        if self.proj:\n",
        "            self.norm0 = SPADE(in_channels, cond_channels)\n",
        "            self.conv0 = nn.utils.spectral_norm(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "            )\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.norm1 = SPADE(in_channels, cond_channels)\n",
        "        self.norm2 = SPADE(hid_channels, cond_channels)\n",
        "        self.conv1 = nn.utils.spectral_norm(\n",
        "            nn.Conv2d(in_channels, hid_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.conv2 = nn.utils.spectral_norm(\n",
        "            nn.Conv2d(hid_channels, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        dx = self.norm1(x, seg)\n",
        "        dx = self.activation(dx)\n",
        "        dx = self.conv1(dx)\n",
        "        dx = self.norm2(dx, seg)\n",
        "        dx = self.activation(dx)\n",
        "        dx = self.conv2(dx)\n",
        "\n",
        "        # Learn skip connection if in_channels != out_channels\n",
        "        if self.proj:\n",
        "            x = self.norm0(x, seg)\n",
        "            x = self.conv0(x)\n",
        "\n",
        "        return x + dx"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8PjnPm142YM"
      },
      "source": [
        "## GauGAN Parts\n",
        "\n",
        "Now that you understand the main contributions of GauGAN and its submodules, let's dive into the encoder, generator, and discriminator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F38Et-m3_1B"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "GauGAN's encoder serves a different purpose than Pix2PixHD's. Instead of learning feature maps to be fed as input to the generator, GauGAN's encoder encodes the original image into a mean and standard deviation from which to sample noise, which is given to the generator. You may recall this same technique of encoding to a mean and standard devation is used in variational autoencoders (VAEs, covered in a Course 2 optional notebook). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfLUrDS440vm"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder Class\n",
        "    Values:\n",
        "        spatial_size: tuple specifying (height, width) of full size image, a tuple\n",
        "        z_dim: number of dimensions of latent noise vector (z), a scalar\n",
        "        n_downsample: number of downsampling blocks in the encoder, a scalar\n",
        "        base_channels: number of channels in the last hidden layer, a scalar\n",
        "    '''\n",
        "\n",
        "    max_channels = 512\n",
        "\n",
        "    def __init__(self, spatial_size, z_dim=256, n_downsample=6, base_channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        channels = base_channels\n",
        "        for i in range(n_downsample):\n",
        "            in_channels = 3 if i == 0 else channels\n",
        "            out_channels = 2 * z_dim if i < n_downsample else max(self.max_channels, channels * 2)\n",
        "            layers += [\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(in_channels, out_channels, stride=2, kernel_size=3, padding=1)\n",
        "                ),\n",
        "                nn.InstanceNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            ]\n",
        "            channels = out_channels\n",
        "\n",
        "        h, w = spatial_size[0] // 2 ** n_downsample, spatial_size[1] // 2 ** n_downsample\n",
        "        layers += [\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels * h * w, 2 * z_dim),\n",
        "        ]\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.chunk(self.layers(x), 2, dim=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKIst9xTs7OI"
      },
      "source": [
        "### Generator\n",
        "\n",
        "The GauGAN generator is actually very different from previous image-to-image translation generators. Because information from the semantic label map is injected at each batch normalization layer, the generator is able to just take random noise $z$ as input. This noise is reshaped and upsampled to the target image size.\n",
        "\n",
        "Let's take a look at the implementation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqgMm3Gs0B0a"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        n_classes: the number of object classes in the dataset, a scalar\n",
        "        bottom_width: the downsampled spatial size of the image, a scalar\n",
        "        z_dim: the number of dimensions the z noise vector has, a scalar\n",
        "        base_channels: the number of channels in last hidden layer, a scalar\n",
        "        n_upsample: the number of upsampling operations to apply, a scalar\n",
        "    '''\n",
        "\n",
        "    max_channels = 1024\n",
        "\n",
        "    def __init__(self, n_classes, spatial_size, z_dim=256, base_channels=64, n_upsample=6):\n",
        "        super().__init__()\n",
        "\n",
        "        h, w = spatial_size[0] // 2 ** n_upsample, spatial_size[1] // 2 ** n_upsample\n",
        "        self.proj_z = nn.Linear(z_dim, self.max_channels * h * w)\n",
        "        self.reshape = lambda x: torch.reshape(x, (-1, self.max_channels, h, w))\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        for i in reversed(range(n_upsample)):\n",
        "            in_channels = min(self.max_channels, base_channels * 2 ** (i+1))\n",
        "            out_channels = min(self.max_channels, base_channels * 2 ** i)\n",
        "            self.res_blocks.append(ResidualBlock(in_channels, out_channels, n_classes))\n",
        "\n",
        "        self.proj_o = nn.Sequential(\n",
        "            nn.Conv2d(base_channels, 3, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z, seg):\n",
        "        h = self.proj_z(z)\n",
        "        h = self.reshape(h)\n",
        "        for res_block in self.res_blocks:\n",
        "            h = res_block(h, seg)\n",
        "            h = self.upsample(h)\n",
        "        h = self.proj_o(h)\n",
        "        return h"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HoyIbXajoMC"
      },
      "source": [
        "### Discriminator\n",
        "\n",
        "The architecture of the discriminator follows the one used in Pix2PixHD, which uses a multi-scale design with the InstanceNorm. The only difference here is that they apply spectral normalization to all convolutional layers. GauGAN's discriminator also takes as input the image concatenated with the semantic label map (no instance boundary map as in Pix2PixHD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYgX2B_hDxkA"
      },
      "source": [
        "class PatchGANDiscriminator(nn.Module):\n",
        "    '''\n",
        "    PatchGANDiscriminator Class\n",
        "    Implements the discriminator class for a subdiscriminator, \n",
        "    which can be used for all the different scales, just with different argument values.\n",
        "    Values:\n",
        "        in_channels: the number of channels in input, a scalar\n",
        "        base_channels: the number of channels in first convolutional layer, a scalar\n",
        "        n_layers: the number of convolutional layers, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, base_channels=64, n_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Use nn.ModuleList so we can output intermediate values for loss.\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.layers.append(\n",
        "            nn.Sequential(\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(in_channels, base_channels, kernel_size=4, stride=2, padding=2)\n",
        "                ),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Downsampling convolutional layers\n",
        "        channels = base_channels\n",
        "        for _ in range(1, n_layers):\n",
        "            prev_channels = channels\n",
        "            channels = min(2 * channels, 512)\n",
        "            self.layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.utils.spectral_norm(\n",
        "                        nn.Conv2d(prev_channels, channels, kernel_size=4, stride=2, padding=2)\n",
        "                    ),\n",
        "                    nn.InstanceNorm2d(channels, affine=False),\n",
        "                    nn.LeakyReLU(0.2, inplace=True),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Output convolutional layer\n",
        "        prev_channels = channels\n",
        "        channels = min(2 * channels, 512)\n",
        "        self.layers.append(\n",
        "            nn.Sequential(\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(prev_channels, channels, kernel_size=4, stride=1, padding=2))\n",
        "                ,\n",
        "                nn.InstanceNorm2d(channels, affine=False),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 1, kernel_size=4, stride=1, padding=2)\n",
        "                ),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [] # for feature matching loss\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            outputs.append(x)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZchSdgY1Jrd5"
      },
      "source": [
        "Now you're ready to implement the multiscale discriminator in full! This puts together the different subdiscriminator scales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_kqcsh4Jwjz"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    Discriminator Class\n",
        "    Values:\n",
        "        in_channels: number of input channels to each discriminator, a scalar\n",
        "        base_channels: number of channels in last hidden layer, a scalar\n",
        "        n_layers: number of downsampling layers in each discriminator, a scalar\n",
        "        n_discriminators: number of discriminators at different scales, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, base_channels=64, n_layers=3, n_discriminators=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize all discriminators\n",
        "        self.discriminators = nn.ModuleList()\n",
        "        for _ in range(n_discriminators):\n",
        "            self.discriminators.append(\n",
        "                PatchGANDiscriminator(in_channels, base_channels=base_channels, n_layers=n_layers)\n",
        "            )\n",
        "\n",
        "        # Downsampling layer to pass inputs between discriminators at different scales\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "\n",
        "        for i, discriminator in enumerate(self.discriminators):\n",
        "            # Downsample input for subsequent discriminators\n",
        "            if i != 0:\n",
        "                x = self.downsample(x)\n",
        "\n",
        "            outputs.append(discriminator(x))\n",
        "\n",
        "        # Return list of multiscale discriminator outputs\n",
        "        return outputs\n",
        "\n",
        "    @property\n",
        "    def n_discriminators(self):\n",
        "        return len(self.discriminators)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vsz1XL4yjuh"
      },
      "source": [
        "## GauGAN: Putting it all together\n",
        "\n",
        "You can now create your GauGAN model that encapsulates all the parts you've just learned about! Since the encoder outputs mean and log-variance values to sample random noise from, this implementation will use the 'reparameterization trick' to allow gradient flow to the encoder. If you're not familiar with this trick, it samples from $\\mathcal{N}(0, I)$ and applies shift and scale ($\\mu, \\sigma$) as opposed to sampling directly from $\\mathcal{N}(\\mu, \\sigma^2I)$:\n",
        "\n",
        "\\begin{align*}\n",
        "    z: z \\sim \\mathcal{N}(\\mu, \\sigma^2I) \\equiv \\sigma * z + \\mu: z \\sim \\mathcal{N}(0, I).\n",
        "\\end{align*}\n",
        "\n",
        "Check out the optional VAE notebook for a more detailed description!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slAXJc_wzxVT"
      },
      "source": [
        "class GauGAN(nn.Module):\n",
        "    '''\n",
        "    GauGAN Class\n",
        "    Values:\n",
        "        n_classes: number of object classes in dataset, a scalar\n",
        "        spatial_size: tuple containing (height, width) of full-size image, a tuple\n",
        "        base_channels: number of channels in last generator & first discriminator layers, a scalar\n",
        "        z_dim: number of dimensions in noise vector (z), a scalar\n",
        "        n_upsample: number of downsampling (encoder) and upsampling (generator) operations, a scalar\n",
        "        n_disc_layer:: number of discriminator layers, a scalar\n",
        "        n_disc: number of discriminators (at different scales), a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes,\n",
        "        spatial_size,\n",
        "        base_channels=64,\n",
        "        z_dim=256,\n",
        "        n_upsample=6,\n",
        "        n_disc_layers=3,\n",
        "        n_disc=3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            spatial_size, z_dim=z_dim, n_downsample=n_upsample, base_channels=base_channels,\n",
        "        )\n",
        "        self.generator = Generator(\n",
        "            n_classes, spatial_size, z_dim=z_dim, base_channels=base_channels, n_upsample=n_upsample,\n",
        "        )\n",
        "        self.discriminator = Discriminator(\n",
        "            n_classes + 3, base_channels=base_channels, n_layers=n_disc_layers, n_discriminators=n_disc,\n",
        "        )\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        ''' Performs a full forward pass for training. '''\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.sample_z(mu, logvar)\n",
        "        x_fake = self.generate(z, seg)\n",
        "        pred = self.discriminate(x_fake, seg)\n",
        "        return x_fake, pred\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def generate(self, z, seg):\n",
        "        ''' Generates fake image from noise vector and segmentation. '''\n",
        "        return self.generator(z, seg)\n",
        "\n",
        "    def discriminate(self, x, seg):\n",
        "        ''' Predicts whether input image is real. '''\n",
        "        return self.discriminator(torch.cat((x, seg), dim=1))\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_z(mu, logvar):\n",
        "        ''' Samples noise vector with reparameterization trick. '''\n",
        "        eps = torch.randn(mu.size(), device=mu.device).to(mu.dtype)\n",
        "        return (logvar / 2).exp() * eps + mu\n",
        "\n",
        "    @property\n",
        "    def n_disc(self):\n",
        "        return self.discriminator.n_discriminators"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYlQTBpkkWkd"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "GauGAN reuses the composite loss functions that Pix2PixHD does, except it replaces the LSGAN loss with [Hinge loss](https://paperswithcode.com/method/gan-hinge-loss). It also imposes a soft (0.05 weight) Kullbach-Leibler divergence (KLD) loss term on the Gaussian statistics generated by the encoder.\n",
        "\n",
        "#### A debrief on KLD\n",
        "\n",
        "KLD measures how different two probability distributions are. In the case of $\\mathcal{N}(\\mu, \\sigma^2I)$ learned by the encoder, KLD loss encourages the learned distribution to be close to a standard Gaussian. For more information on implementation, check out Pytorch's [KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCXhrrsudbB2"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    '''\n",
        "    VGG19 Class\n",
        "    Wrapper for pretrained torchvision.models.vgg19 to output intermediate feature maps\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg_features = models.vgg19(pretrained=True).features\n",
        "\n",
        "        self.f1 = nn.Sequential(*[vgg_features[x] for x in range(2)])\n",
        "        self.f2 = nn.Sequential(*[vgg_features[x] for x in range(2, 7)])\n",
        "        self.f3 = nn.Sequential(*[vgg_features[x] for x in range(7, 12)])\n",
        "        self.f4 = nn.Sequential(*[vgg_features[x] for x in range(12, 21)])\n",
        "        self.f5 = nn.Sequential(*[vgg_features[x] for x in range(21, 30)])\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = self.f1(x)\n",
        "        h2 = self.f2(h1)\n",
        "        h3 = self.f3(h2)\n",
        "        h4 = self.f4(h3)\n",
        "        h5 = self.f5(h4)\n",
        "        return [h1, h2, h3, h4, h5]\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    '''\n",
        "    Loss Class\n",
        "    Implements composite loss for GauGAN\n",
        "    Values:\n",
        "        lambda1: weight for feature matching loss, a float\n",
        "        lambda2: weight for vgg perceptual loss, a float\n",
        "        lambda3: weight for KLD loss, a float\n",
        "        device: 'cuda' or 'cpu' for hardware to use\n",
        "        norm_weight_to_one: whether to normalize weights to (0, 1], a bool\n",
        "    '''\n",
        "\n",
        "    def __init__(self, lambda1=10., lambda2=10., lambda3=0.05, device='cuda', norm_weight_to_one=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vgg = VGG19().to(device)\n",
        "        self.vgg_weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "\n",
        "        lambda0 = 1.0\n",
        "        # Keep ratio of composite loss, but scale down max to 1.0\n",
        "        scale = max(lambda0, lambda1, lambda2, lambda3) if norm_weight_to_one else 1.0\n",
        "\n",
        "        self.lambda0 = lambda0 / scale\n",
        "        self.lambda1 = lambda1 / scale\n",
        "        self.lambda2 = lambda2 / scale\n",
        "        self.lambda3 = lambda3 / scale\n",
        "\n",
        "    def kld_loss(self, mu, logvar):\n",
        "        return -0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp())\n",
        "\n",
        "    def g_adv_loss(self, discriminator_preds):\n",
        "        adv_loss = 0.0\n",
        "        for preds in discriminator_preds:\n",
        "            pred = preds[-1]\n",
        "            adv_loss += -pred.mean()\n",
        "        return adv_loss\n",
        "\n",
        "    def d_adv_loss(self, discriminator_preds, is_real):\n",
        "        adv_loss = 0.0\n",
        "        for preds in discriminator_preds:\n",
        "            pred = preds[-1]\n",
        "            target = -1 + pred if is_real else -1 - pred\n",
        "            mask = target < 0\n",
        "            adv_loss += (mask * target).mean()\n",
        "        return adv_loss\n",
        "\n",
        "    def fm_loss(self, real_preds, fake_preds):\n",
        "        fm_loss = 0.0\n",
        "        for real_features, fake_features in zip(real_preds, fake_preds):\n",
        "            for real_feature, fake_feature in zip(real_features, fake_features):\n",
        "                fm_loss += F.l1_loss(real_feature.detach(), fake_feature)\n",
        "        return fm_loss\n",
        "\n",
        "    def vgg_loss(self, x_real, x_fake):\n",
        "        vgg_real = self.vgg(x_real)\n",
        "        vgg_fake = self.vgg(x_fake)\n",
        "\n",
        "        vgg_loss = 0.0\n",
        "        for real, fake, weight in zip(vgg_real, vgg_fake, self.vgg_weights):\n",
        "            vgg_loss += weight * F.l1_loss(real.detach(), fake)\n",
        "        return vgg_loss\n",
        "\n",
        "    def forward(self, x_real, label_map, gaugan):\n",
        "        '''\n",
        "        Function that computes the forward pass and total loss for GauGAN.\n",
        "        '''\n",
        "        mu, logvar = gaugan.encode(x_real)\n",
        "        z = gaugan.sample_z(mu, logvar)\n",
        "        x_fake = gaugan.generate(z, label_map)\n",
        "\n",
        "        # Get necessary outputs for loss/backprop for both generator and discriminator\n",
        "        fake_preds_for_g = gaugan.discriminate(x_fake, label_map)\n",
        "        fake_preds_for_d = gaugan.discriminate(x_fake.detach(), label_map)\n",
        "        real_preds_for_d = gaugan.discriminate(x_real.detach(), label_map)\n",
        "\n",
        "        g_loss = (\n",
        "            self.lambda0 * self.g_adv_loss(fake_preds_for_g) + \\\n",
        "            self.lambda1 * self.fm_loss(real_preds_for_d, fake_preds_for_g) / gaugan.n_disc + \\\n",
        "            self.lambda2 * self.vgg_loss(x_fake, x_real) + \\\n",
        "            self.lambda3 * self.kld_loss(mu, logvar)\n",
        "        )\n",
        "        d_loss = 0.5 * (\n",
        "            self.d_adv_loss(real_preds_for_d, True) + \\\n",
        "            self.d_adv_loss(fake_preds_for_d, False)\n",
        "        )\n",
        "\n",
        "        return g_loss, d_loss, x_fake.detach()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcyavR5oLmau"
      },
      "source": [
        "## Training GauGAN\n",
        "\n",
        "You now have GauGAN implemented! All you have to do now is prepare your dataset. The authors trained GauGAN on a variety of datasets, but for simplicity, this notebook is geared towards the Cityscapes dataset (Pix2PixHD is also trained on the Cityscapes dataset). You'll have to download the dataset, unzip it, and put it in your `data` folder to initialize the dataset code below.\n",
        "\n",
        "Specifically, you should download the `gtFine_trainvaltest` and `leftImg8bit_trainvaltest` and specify the corresponding data splits into the dataloader.\n",
        "\n",
        "Below is a quick dataset class to help you load and preprocess the Cityscapes examples. You can also run the code below to download the dataset, after you've created an account at http://cityscapes-dataset.com."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAjgcHDS64_o"
      },
      "source": [
        "# Store necessary cookies\n",
        "import getpass, os\n",
        "username = input(\"What is your Cityscapes username? (http://cityscapes-dataset.com) \")\n",
        "password = getpass.getpass(\"What is your Cityscapes password? \")\n",
        "os.mkdir(\"data\")\n",
        "os.system(f\"wget --keep-session-cookies --save-cookies=data/cookies.txt --post-data 'username={username}&password={password}&submit=Login' https://www.cityscapes-dataset.com/login/\")\n",
        "# Download data\n",
        "!cd data; wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1\n",
        "!cd data; wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3\n",
        "# Unzip data\n",
        "!cd data; unzip leftImg8bit_trainvaltest\n",
        "!cd data; unzip gtFine_trainvaltest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mII5L2cZLlpO"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CityscapesDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    CityscapesDataset Class\n",
        "    Values:\n",
        "        paths: (a list of) paths to construct dataset from, a list or string\n",
        "        img_size: tuple containing the (height, width) for resizing, a tuple\n",
        "        n_classes: the number of object classes, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, paths, img_size=(256, 512), n_classes=35):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Collect list of examples\n",
        "        self.examples = {}\n",
        "        if type(paths) == str:\n",
        "            self.load_examples_from_dir(paths)\n",
        "        elif type(paths) == list:\n",
        "            for path in paths:\n",
        "                self.load_examples_from_dir(path)\n",
        "        else:\n",
        "            raise ValueError('`paths` should be a single path or list of paths')\n",
        "\n",
        "        self.examples = list(self.examples.values())\n",
        "        assert all(len(example) == 2 for example in self.examples)\n",
        "\n",
        "        # Initialize transforms for the real color image\n",
        "        self.img_transforms = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.Lambda(lambda img: np.array(img)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "\n",
        "        # Initialize transforms for semantic label maps\n",
        "        self.map_transforms = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.Lambda(lambda img: np.array(img)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def load_examples_from_dir(self, abs_path):\n",
        "        '''\n",
        "        Given a folder of examples, this function returns a list of paired examples.\n",
        "        '''\n",
        "        assert os.path.isdir(abs_path)\n",
        "\n",
        "        img_suffix = '_leftImg8bit.png'\n",
        "        label_suffix = '_gtFine_labelIds.png'\n",
        "\n",
        "        for root, _, files in os.walk(abs_path):\n",
        "            for f in files:\n",
        "                if f.endswith(img_suffix):\n",
        "                    prefix = f[:-len(img_suffix)]\n",
        "                    attr = 'orig_img'\n",
        "                elif f.endswith(label_suffix):\n",
        "                    prefix = f[:-len(label_suffix)]\n",
        "                    attr = 'label_map'\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if prefix not in self.examples.keys():\n",
        "                    self.examples[prefix] = {}\n",
        "                self.examples[prefix][attr] = root + '/' + f\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "\n",
        "        # Load image and maps\n",
        "        img = Image.open(example['orig_img']).convert('RGB') # color image: (3, h, w)\n",
        "        label = Image.open(example['label_map'])             # semantic label map: (1, h, w)\n",
        "\n",
        "        # Apply corresponding transforms\n",
        "        img = self.img_transforms(img)\n",
        "        label = self.map_transforms(label).long() * 255\n",
        "\n",
        "        # Convert labels to one-hot vectors\n",
        "        label = F.one_hot(label, num_classes=self.n_classes)\n",
        "        label = label.squeeze(0).permute(2, 0, 1).to(img.dtype)\n",
        "\n",
        "        return (img, label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        imgs, labels = [], []\n",
        "        for (x, l) in batch:\n",
        "            imgs.append(x)\n",
        "            labels.append(l)\n",
        "        return torch.stack(imgs, dim=0), torch.stack(labels, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51StN80TCpgE"
      },
      "source": [
        "Now initialize everything you'll need for training. Don't be worried if there looks like a lot of random code, it's all stuff you've seen before!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG1ZIAI8DgJX"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    ''' Function for scheduling learning rate '''\n",
        "    return 1. if epoch < decay_after else 1 - float(epoch - decay_after) / (epochs - decay_after)\n",
        "\n",
        "def weights_init(m):\n",
        "    ''' Function for initializing all model weights '''\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weights_init)\n",
        "\n",
        "# Initialize model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gaugan_config = {\n",
        "    'n_classes': 35,\n",
        "    'spatial_size': (128, 256), # Default (256, 512): halve size for memory\n",
        "    'base_channels': 32,        # Default 64: halve channels for memory\n",
        "    'z_dim': 256,\n",
        "    'n_upsample': 5,            # Default 6: decrease layers for memory\n",
        "    'n_disc_layers': 2,\n",
        "    'n_disc': 3,\n",
        "}\n",
        "gaugan = GauGAN(**gaugan_config).to(device)\n",
        "loss = Loss(device=device)\n",
        "\n",
        "# Initialize dataloader\n",
        "train_dir = ['data']\n",
        "batch_size = 16                 # Default 32: decrease for memory\n",
        "dataset = CityscapesDataset(\n",
        "    train_dir, img_size=gaugan_config['spatial_size'], n_classes=gaugan_config['n_classes'],\n",
        ")\n",
        "dataloader = DataLoader(\n",
        "    dataset, collate_fn=CityscapesDataset.collate_fn,\n",
        "    batch_size=batch_size, shuffle=True,\n",
        "    drop_last=False, pin_memory=True,\n",
        ")\n",
        "\n",
        "# Initialize optimizers + schedulers\n",
        "epochs = 200                    # total number of train epochs\n",
        "decay_after = 100               # number of epochs with constant lr\n",
        "betas = (0.0, 0.999)\n",
        "\n",
        "g_params = list(gaugan.generator.parameters()) + list(gaugan.encoder.parameters())\n",
        "d_params = list(gaugan.discriminator.parameters())\n",
        "\n",
        "g_optimizer = torch.optim.Adam(g_params, lr=1e-4, betas=betas)\n",
        "d_optimizer = torch.optim.Adam(d_params, lr=4e-4, betas=betas)\n",
        "g_scheduler = torch.optim.lr_scheduler.LambdaLR(g_optimizer, lr_lambda)\n",
        "d_scheduler = torch.optim.lr_scheduler.LambdaLR(d_optimizer, lr_lambda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwyO_padKxW5"
      },
      "source": [
        "And now the training loop, which is pretty much the same between the two phases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-2lU4TnK5ik"
      },
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parse torch version for autocast\n",
        "# #################################################\n",
        "version = torch.__version__\n",
        "version = tuple(int(n) for n in version.split('.')[:-1])\n",
        "has_autocast = version >= (1, 6)\n",
        "# #################################################\n",
        "\n",
        "def show_tensor_images(image_tensor):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:1], nrow=1)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def train(dataloader, gaugan, optimizers, schedulers, device):\n",
        "    g_optimizer, d_optimizer = optimizers\n",
        "    g_scheduler, d_scheduler = schedulers\n",
        "\n",
        "    cur_step = 0\n",
        "    display_step = 100\n",
        "\n",
        "    mean_g_loss = 0.0\n",
        "    mean_d_loss = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training epoch\n",
        "        for (x_real, labels) in tqdm(dataloader, position=0):\n",
        "            x_real = x_real.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
        "            # If you're running older versions of torch, comment this out\n",
        "            # and use NVIDIA apex for mixed/half precision training\n",
        "            if has_autocast:\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    g_loss, d_loss, x_fake = loss(x_real, labels, gaugan)\n",
        "            else:\n",
        "                g_loss, d_loss, x_fake = loss(x_real, labels, gaugan)\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            d_optimizer.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            mean_g_loss += g_loss.item() / display_step\n",
        "            mean_d_loss += d_loss.item() / display_step\n",
        "\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                print('Step {}: Generator loss: {:.5f}, Discriminator loss: {:.5f}'\n",
        "                      .format(cur_step, mean_g_loss, mean_d_loss))\n",
        "                show_tensor_images(x_fake.to(x_real.dtype))\n",
        "                show_tensor_images(x_real)\n",
        "                mean_g_loss = 0.0\n",
        "                mean_d_loss = 0.0\n",
        "            cur_step += 1\n",
        "\n",
        "        g_scheduler.step()\n",
        "        d_scheduler.step()\n",
        "\n",
        "train(\n",
        "    dataloader, gaugan,\n",
        "    [g_optimizer, d_optimizer],\n",
        "    [g_scheduler, d_scheduler],\n",
        "    device,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}